# -*- coding: utf-8 -*-
"""Proyek_Brain_Tumor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yPee4s3iE117b7Z31noc24Jg8hl8aerW

# Instalasi Package
"""

!pip install tensorflow scikit-learn

"""# Import Library"""

# Standard Library
import os
import shutil
import random
import zipfile

# Data Manipulation
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Image Processing
from PIL import Image
import cv2

# Machine Learning & Metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# TensorFlow and Keras
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras import regularizers

# Google Colab
from google.colab import files

"""# Mount Google Drive dan Ekstrak Dataset"""

from google.colab import drive
drive.mount('/content/drive')

zip_file_path = '/content/drive/MyDrive/tumor_otak_dataset.zip'

extract_path = '/content/brain_tumor/'

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

for root, dirs, files in os.walk(extract_path):
    for name in files:
        print(os.path.join(root, name))

"""# Menggabungkan Dataset"""

dataset_paths = [
    '/content/brain_tumor/tumor_otak_dataset/dataset_1/Testing',
    '/content/brain_tumor/tumor_otak_dataset/dataset_1/Training',
    '/content/brain_tumor/tumor_otak_dataset/dataset_2/Testing',
    '/content/brain_tumor/tumor_otak_dataset/dataset_2/Training',
]

combined_dir = '/content/brain_tumor/tumor_otak_dataset/combined'

classes = ['glioma', 'meningioma', 'notumor', 'pituitary']
for cls in classes:
    os.makedirs(os.path.join(combined_dir, cls), exist_ok=True)

for path in dataset_paths:
    for cls in classes:
        src_dir = os.path.join(path, cls)
        dst_dir = os.path.join(combined_dir, cls)
        if os.path.exists(src_dir):
            for fname in os.listdir(src_dir):
                src_file = os.path.join(src_dir, fname)
                dst_file = os.path.join(dst_dir, fname)
                shutil.copy2(src_file, dst_file)

"""# Visualisasi Sampel Gambar"""

combined_dir = '/content/brain_tumor/tumor_otak_dataset/combined'

combined_paths = []
combined_labels = []

for path, subdirs, files in os.walk(combined_dir):
    for name in files:
        combined_paths.append(os.path.join(path, name))
        combined_labels.append(os.path.basename(path))

random_choice = random.sample(range(len(combined_paths)), 12)

fig, axes = plt.subplots(3, 4, figsize=(15, 8))
axes = axes.ravel()

for i, idx in enumerate(random_choice):
    img_path = combined_paths[idx]
    img = Image.open(img_path)
    img = img.resize((224, 224))

    axes[i].imshow(img)
    axes[i].axis('off')
    axes[i].set_title(f"Label: {combined_labels[idx]}", fontsize=10)

plt.tight_layout()
plt.show()

"""# Statistik Dataset"""

distribution_df = pd.DataFrame({'path': combined_paths, 'label': combined_labels})

plt.figure(figsize=(6,6))
sns.set_style("darkgrid")
sns.countplot(data=distribution_df, x="label")
plt.title("Distribusi Gambar per Kelas di Combined Folder")
plt.xlabel("Kelas")
plt.ylabel("Jumlah Gambar")
plt.xticks(rotation=45)
plt.show()

distribution_df = pd.DataFrame({'path': combined_paths, 'label': combined_labels})

class_counts = distribution_df['label'].value_counts()
print("Jumlah data per kelas di folder Combined:")
print(class_counts)

"""# **Split Dataset** - Train, Validation, Test



"""

combined_dir = '/content/brain_tumor/tumor_otak_dataset/combined'

split_dataset_dir = "/content/brain_tumor/split"
train_dir = os.path.join(split_dataset_dir, "train")
val_dir = os.path.join(split_dataset_dir, "val")
test_dir = os.path.join(split_dataset_dir, "test")

for split_dir in [train_dir, val_dir, test_dir]:
    os.makedirs(split_dir, exist_ok=True)

val_ratio = 0.2
test_ratio = 0.1

for class_name in os.listdir(combined_dir):
    class_path = os.path.join(combined_dir, class_name)
    if not os.path.isdir(class_path):
        continue

    files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]

    train_val_files, test_files = train_test_split(files, test_size=test_ratio, random_state=42)

    val_size_adjusted = val_ratio / (1 - test_ratio)  # 0.2 / 0.9
    train_files, val_files = train_test_split(train_val_files, test_size=val_size_adjusted, random_state=42)

    def salin(files, destination_dir):
        os.makedirs(os.path.join(destination_dir, class_name), exist_ok=True)
        for file in files:
            src = os.path.join(class_path, file)
            dst = os.path.join(destination_dir, class_name, file)
            shutil.copy2(src, dst)

    salin(train_files, train_dir)
    salin(val_files, val_dir)
    salin(test_files, test_dir)

print("Dataset berhasil dibagi: 70% train, 20% validation, 10% test.")

for split in ['train', 'val', 'test']:
    split_path = os.path.join(split_dataset_dir, split)
    print(f"\n{split.upper()} SET:")
    for class_name in os.listdir(split_path):
        class_path = os.path.join(split_path, class_name)
        num_images = len(os.listdir(class_path))
        print(f"- {class_name}: {num_images} images")

"""# Visualisasi Distribusi Data per Split"""

split_counts = {'train': [], 'val': [], 'test': []}
classes = os.listdir(train_dir)

for split in ['train', 'val', 'test']:
    for class_name in classes:
        path = os.path.join(split_dataset_dir, split, class_name)
        count = len([f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])
        split_counts[split].append(count)

df = pd.DataFrame(split_counts, index=classes)

df.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'orange', 'lightgreen'])
plt.title('Distribusi Jumlah Gambar per Kelas dan Split')
plt.xlabel('Kelas')
plt.ylabel('Jumlah Gambar')
plt.xticks(rotation=45)
plt.legend(title="Split")
plt.tight_layout()
plt.show()

"""# Data Augmentation & Generator"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Validation hanya rescale
val_datagen = ImageDataGenerator(rescale=1./255)

# Load data
train_data = train_datagen.flow_from_directory(
    '/content/brain_tumor/split/train',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

val_data = val_datagen.flow_from_directory(
    '/content/brain_tumor/split/val',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

"""# Model VGG16 (Transfer Learning)"""

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = BatchNormalization()(x)
x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)
x = Dropout(0.4)(x)
x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)
x = Dropout(0.4)(x)
predictions = Dense(4, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),
    ModelCheckpoint('best_vgg16_model.keras', save_best_only=True, monitor='val_loss')
]

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    callbacks=callbacks
)

for layer in base_model.layers[-8:]:
    layer.trainable = True

model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_fine = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    callbacks=callbacks
)

"""# Visualisasi Training History"""

def plot_training(history, history_fine=None):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    if history_fine:
        acc += history_fine.history['accuracy']
        val_acc += history_fine.history['val_accuracy']
        loss += history_fine.history['loss']
        val_loss += history_fine.history['val_loss']

    epochs_range = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training & Validation Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training & Validation Loss')

    plt.tight_layout()
    plt.show()

plot_training(history, history_fine)

"""# Evaluasi Model (Data Uji)

"""

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    '/content/brain_tumor/split/test',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)
print(f"\nTest Accuracy: {test_accuracy*100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

"""# Menyimpan dan Menampilkan Evaluasi Akurasi & Loss"""

results = {
    "Set": ["Training", "Validation", "Testing"],
    "Accuracy": [0.9822, 0.9680, 0.9744],
    "Loss": [0.1165, 0.1427, 0.1359]
}

eval_df = pd.DataFrame(results)
print(eval_df)

"""# Confusion Matrix"""

classes = ['glioma', 'meningioma', 'notumor', 'pituitary']

y_true = test_generator.classes
y_pred_probabilities = model.predict(test_generator)
y_pred = np.argmax(y_pred_probabilities, axis=1)

class_indices = test_generator.class_indices
idx_to_class = {v: k for k, v in class_indices.items()}

print("Class Indices from test_generator:", test_generator.class_indices)

cm = confusion_matrix(y_true, y_pred, labels=list(class_indices.values()))

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""# Classification Report"""

classes = ['glioma', 'meningioma', 'notumor', 'pituitary']

report = classification_report(y_true, y_pred, target_names=classes)
print("Classification Report:\n", report)

from google.colab import files
model.save('brain_tumor_model.keras')
files.download('brain_tumor_model.keras')

from google.colab import files
model.save('brain_tumor_model.h5')
files.download('brain_tumor_model.h5')

import shutil
shutil.make_archive('brain_tumor_model', 'zip', 'brain_tumor_model')

from google.colab import files
files.download('brain_tumor_model.zip')

import tensorflow as tf
print(tf.__version__)

